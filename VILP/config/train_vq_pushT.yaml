_target_: VILP.workspace.train_vqgan_workspace.TrainVqganWorkspace
base_learning_rate: 4.5e-6
model:
  embed_dim: 3
  n_embed: 1024
  ddconfig:
    double_z: False
    z_channels: 3
    resolution: 96
    in_channels: 3
    out_ch: 3
    ch: 128
    ch_mult: [1,1,2,4]  # num_down = len(ch_mult)-1
    num_res_blocks: 2
    attn_resolutions: [16]
    dropout: 0.0
  lossconfig:
    target: VILP.taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
    params:
      disc_conditional: False
      disc_in_channels: 3
      #disc_start: 10000
      # for sim push-T, due to its white background, we do not need to use the discriminator
      # to help reconstruct image details
      disc_start: 1000000
      disc_weight: 0.8
      codebook_weight: 1.0
dataset:
  _target_: VILP.dataset.push_multi_image_ae_dataset.PushMultiImageAeDataset
  zarr_path: data/pusht/pusht_cchi_v7_replay.zarr
  horizon: 1
  max_train_episodes: null
  pad_after: 7
  pad_before: 1
  seed: 42
  val_ratio: 0.00
val_dataloader:
  batch_size: 16
  num_workers: 4
  persistent_workers: false
  pin_memory: true
  shuffle: false
dataloader:
  batch_size: 16
  num_workers: 4
  persistent_workers: false
  pin_memory: true
  shuffle: true
gpus: '0,'
batch_size: 8
seed: 42
trainer:
  name: "vqvae_debug"
  resume: ""
  base: []
  no_test: false
  project: null
  debug: false
  seed: 42
  postfix: ""
  train: true
save_every: 1
max_epochs: 100000